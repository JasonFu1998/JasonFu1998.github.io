---
---

@inproceedings{10.1117/12.2532494,
  abstract = {Panoramic images have advantages in information capacity and scene stability due to their large field of view (FoV). In this paper, we propose a method to synthesize a new dataset of panoramic image. We managed to stitch the images taken from different directions into panoramic images, together with their labeled images, to yield the panoramic semantic segmentation dataset denominated as SYNTHIA-PANO. For the purpose of finding out the effect of using panoramic images as training dataset, we designed and performed a comprehensive set of experiments. Experimental results show that using panoramic images as training data is beneficial to the segmentation result. In addition, it has been shown that by using panoramic images with a 180 degree FoV as training data the model has better performance. Furthermore, the model trained with panoramic images also has a better capacity to resist the image distortion. Our codes and SYNTHIA-PANO dataset are available: https://github.com/Francis515/SYNTHIA-PANO.},
  author = {Y. Xu and K. Wang and K. Yang and D. Sun and J. Fu},
  title = {Semantic segmentation of panoramic images using a synthetic dataset},
  volume = {11169},
  booktitle = {SPIE Security + Defence Symposium, Strasbourg, France,},
  editor = {Judith Dijk},
  organization = {International Society for Optics and Photonics},
  publisher = {SPIE},
  pages = {90 -- 104},
  keywords = {semantic segmentation, panoramic image, synthetic dataset, SYNTHIA dataset , field of view, cylindrical projection, SYNTHIA-PANO dataset},
  month = {September},
  year = {2019},
  doi = {10.1117/12.2532494},
  URL = {https://doi.org/10.1117/12.2532494},
  html = {https://www.spiedigitallibrary.org/conference-proceedings-of-spie/11169/111690B/Semantic-segmentation-of-panoramic-images-using-a-synthetic-dataset/10.1117/12.2532494.short},
  pdf = {1.pdf},
  image = {seg.png},
  selected = {true}
}

@inproceedings{10.1145/3577190.3614114,
  abstract = {Dance improvisation is an active research topic in the arts. Motion analysis of improvised dance can be challenging due to its unique dynamics. Data-driven dance motion analysis, including recognition and generation, is often limited to skeletal data. However, data of other modalities, such as audio, can be recorded and benefit downstream tasks. This paper explores the application and performance of multimodal fusion methods for human motion recognition in the context of dance improvisation. We propose an attention-based model, component attention network (CANet), for multimodal fusion on three levels: 1) feature fusion with CANet, 2) model fusion with CANet and graph convolutional network (GCN), and 3) late fusion with a voting strategy. We conduct thorough experiments to analyze the impact of each modality in different fusion methods and distinguish critical temporal or component features. We show that our proposed model outperforms the two baseline methods, demonstrating its potential for analyzing improvisation in dance.},
  author = {J. Fu and J. Tan and W. Yin and S. Pashami and M. Bj√∂rkman},
  title = {Component attention network for multimodal dance improvisation recognition},
  volume = {},
  booktitle = {ACM International Conference on Multimodal Interaction, Paris, France,},
  editor = {},
  organization = {},
  publisher = {},
  pages = {},
  keywords = {dance recognition, multimodal fusion, attention network},
  month = {October},
  year = {2023 (10% Oral Session)},
  doi = {10.1145/3577190.3614114},
  URL = {https://doi.org/10.1145/3577190.3614114},
  html = {https://dl.acm.org/doi/10.1145/3577190.3614114},
  pdf = {icmi23-11.pdf},
  image = {10.png},
  selected = {true}
}

@inproceedings{RAVAR,
  abstract = {We introduce a new task called Referring Atomic Video Action Recognition (RAVAR), aimed at identifying atomic actions of a particular person based on a textual description and the video data of this person. This task differs from traditional action recognition and localization, where predictions are delivered for all present individuals. In contrast, we focus on recognizing the correct atomic action of a specific individual, guided by text. To explore this task, we present the RefAVA dataset, containing 36,630 instances with manually annotated textual descriptions of the individuals. To establish a strong initial benchmark, we implement and validate baselines from various domains, e.g., atomic action localization, video question answering, and text-video retrieval. Since these existing methods underperform on RAVAR, we introduce RefAtomNet -- a novel cross-stream attention-driven method specialized for the unique challenges of RAVAR: the need to interpret a textual referring expression for the targeted individual, utilize this reference to guide the spatial localization and harvest the prediction of the atomic actions for the referring person. The key ingredients are: (1) a multi-stream architecture that connects video, text, and a new location-semantic stream, and (2) cross-stream agent attention fusion and agent token fusion which amplify the most relevant information across these streams and consistently surpasses standard attention-based fusion on RAVAR. Extensive experiments demonstrate the effectiveness of RefAtomNet and its building blocks for recognizing the action of the described individual. The dataset and code are publicly available at https://github.com/KPeng9510/RAVAR.},
  author = {K.* Peng and J.* Fu and K. Yang and D. Wen and Y. Chen and R. Liu and J. Zheng and J. Zhang and M. Sarfraz and R. Stiefelhagen and A. Roitberg},
  title = {Referring atomic video action recognition},
  volume = {},
  booktitle = {European Conference on Computer Vision, Milan, Italy,},
  editor = {},
  organization = {},
  publisher = {},
  pages = {},
  keywords = {},
  month = {October},
  year = {2024 (* indicates shared first author)},
  doi = {},
  URL = {},
  html = {https://github.com/KPeng9510/RAVAR},
  pdf = {RAVAR.pdf},
  image = {RAVAR.png},
  selected = {true}
}
